<!-- @format -->

# Project Repository

## Overview

This repository contains multiple projects related to data analysis, machine learning, and information retrieval. The key tools and libraries used include:

- **Python** (Jupyter Notebooks)
- **Pandas & NumPy** (Data Manipulation)
- **Scikit-learn** (Machine Learning Models)
- **Matplotlib & Seaborn** (Data Visualization)
- **PySpark & Google Cloud** (Distributed Computing & MapReduce)
- **NLP & Indexing Techniques** (Wikipedia Parsing, Inverted Index, Ranking)

Each project involves different aspects of data processing, model evaluation, and computational techniques.

---

## Project Descriptions

### **Assignment 1 - Wikipedia Parser & Tokenizer**

- Developed a parser and tokenizer for Wikipedia content.
- Extracted and processed raw text from Wikipedia dumps.
- Used `bz2` for compressed file handling and `Counter` for word frequency analysis.
- Built foundational methods for text processing and NLP tasks.

### **Assignment 2 - Inverted Indexing & Zipf's Law**

- Constructed an **Inverted Index** for efficient search and retrieval.
- Empirically analyzed word frequency distributions and validated **Zipfâ€™s Law**.
- Used `pandas`, `numpy`, and `google.colab.data_table` for structured data representation.
- Implemented different tokenization techniques to improve retrieval performance.

### **Assignment 3 - MapReduce & Web Graph**

- Implemented **MapReduce** using PySpark to scale up indexing operations.
- Processed large-scale datasets in **Google Colab and Google Cloud Platform (GCP)**.
- Created an **Inverted Index** with distributed computing.
- Used `pyspark` for parallel data processing and `graphframes` for web graph analysis.

### **Assignment 4 - Ranking & Evaluation**

- Developed a complete **Retrieval System** based on the **Inverted Index** from Assignment 2.
- Implemented **TF-IDF ranking** and evaluated search effectiveness.
- Used `sklearn.feature_extraction.text.TfidfVectorizer` for text vectorization.
- Computed **cosine similarity** to rank retrieved documents effectively.

---

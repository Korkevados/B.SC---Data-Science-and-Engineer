{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a00e032c",
      "metadata": {
        "id": "a00e032c"
      },
      "source": [
        "***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ac36d3a",
      "metadata": {
        "id": "5ac36d3a",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-Worker_Count",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "cf88b954-f39a-412a-d87e-660833e735b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n",
            "cluster-0016  GCE       2                                             RUNNING  us-central1-a\r\n"
          ]
        }
      ],
      "source": [
        "# if the following command generates an error, you probably didn't enable\n",
        "# the cluster security option \"Allow API access to all Google Cloud services\"\n",
        "# under Manage Security → Project Access when setting up the cluster\n",
        "!gcloud dataproc clusters list --region us-central1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51cf86c5",
      "metadata": {
        "id": "51cf86c5"
      },
      "source": [
        "# Imports & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf199e6a",
      "metadata": {
        "id": "bf199e6a",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-Setup",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "!pip install -q google-cloud-storage==1.43.0 --root-user-action=ignore\n",
        "!pip install -q graphframes --root-user-action=ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8f56ecd",
      "metadata": {
        "id": "d8f56ecd",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-Imports",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "a24aa24b-aa75-4823-83ca-1d7deef0f0de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pyspark\n",
        "import sys\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from google.cloud import storage  # Import the storage module\n",
        "\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38a897f2",
      "metadata": {
        "id": "38a897f2",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-jar",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "8f93a7ec-71e0-49c1-fc81-9af385849a90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 247882 Dec 31 06:35 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"
          ]
        }
      ],
      "source": [
        "# if nothing prints here you forgot to include the initialization script when starting the cluster\n",
        "!ls -l /usr/lib/spark/jars/graph*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47900073",
      "metadata": {
        "id": "47900073",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-pyspark-import",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf, SparkFiles\n",
        "from pyspark.sql import SQLContext\n",
        "from graphframes import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72bed56b",
      "metadata": {
        "id": "72bed56b",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-spark-version",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "07b4e22b-a252-42fb-fe46-d9050e4e7ca8",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - hive</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://cluster-0016-m.c.ir-work-3.internal:42023\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>yarn</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PySparkShell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fd5db529790>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "980e62a5",
      "metadata": {
        "id": "980e62a5",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-bucket_name",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Put your bucket name below and make sure you can access it without an error\n",
        "bucket_name = '315347757work3ir'\n",
        "full_path = f\"gs://{bucket_name}/\"\n",
        "paths=[]\n",
        "\n",
        "client = storage.Client()\n",
        "blobs = client.list_blobs(bucket_name)\n",
        "for b in blobs:\n",
        "    if b.name != 'graphframes.sh':\n",
        "        paths.append(full_path+b.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cac891c2",
      "metadata": {
        "id": "cac891c2"
      },
      "source": [
        "***GCP setup is complete!*** If you got here without any errors you've earned 10 out of the 35 points of this part."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "582c3f5e",
      "metadata": {
        "id": "582c3f5e"
      },
      "source": [
        "# Building an inverted index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "481f2044",
      "metadata": {
        "id": "481f2044"
      },
      "source": [
        "Here, we read the entire corpus to an rdd, directly from Google Storage Bucket and use your code from Colab to construct an inverted index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4c523e7",
      "metadata": {
        "id": "e4c523e7",
        "scrolled": false,
        "outputId": "1e3ed1f7-45c6-4219-f87d-fa51a8f91e3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "parquetFile = spark.read.parquet(*paths)\n",
        "doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d7e2971",
      "metadata": {
        "id": "0d7e2971"
      },
      "source": [
        "We will count the number of pages to make sure we are looking at the entire corpus. The number of pages should be more than 6M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82881fbf",
      "metadata": {
        "id": "82881fbf",
        "outputId": "f97a79fb-a159-4863-aaa7-0d24f1fb24c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "6348910"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Count number of wiki pages\n",
        "parquetFile.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "701811af",
      "metadata": {
        "id": "701811af"
      },
      "source": [
        "Let's import the inverted index module. Note that you need to use the staff-provided version called `inverted_index_gcp.py`, which contains helper functions to writing and reading the posting files similar to the Colab version, but with writing done to a Google Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "121fe102",
      "metadata": {
        "id": "121fe102",
        "outputId": "327fe81b-80f4-4b3a-8894-e74720d92e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inverted_index_gcp.py\r\n"
          ]
        }
      ],
      "source": [
        "# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n",
        "%cd -q /home/dataproc\n",
        "!ls inverted_index_gcp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57c101a8",
      "metadata": {
        "id": "57c101a8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# adding our python module to the cluster\n",
        "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
        "sys.path.insert(0,SparkFiles.getRootDirectory())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c259c402",
      "metadata": {
        "id": "c259c402"
      },
      "outputs": [],
      "source": [
        "from inverted_index_gcp import InvertedIndex"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5540c727",
      "metadata": {
        "id": "5540c727"
      },
      "source": [
        "**YOUR TASK (10 POINTS)**: Use your implementation of `word_count`, `reduce_word_counts`, `calculate_df`, and `partition_postings_and_write` functions from Colab to build an inverted index for all of English Wikipedia in under 2 hours.\n",
        "\n",
        "A few notes:\n",
        "1. The number of corpus stopwords below is a bit bigger than the colab version since we are working on the whole corpus and not just on one file.\n",
        "2. You need to slightly modify your implementation of  `partition_postings_and_write` because the signature of `InvertedIndex.write_a_posting_list` has changed and now includes an additional argument called `bucket_name` for the target bucket. See the module for more details.\n",
        "3. You are not allowed to change any of the code not coming from Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3ad8fea",
      "metadata": {
        "id": "f3ad8fea",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-token2bucket",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
        "                    \"many\", \"however\", \"would\", \"became\"]\n",
        "\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "\n",
        "NUM_BUCKETS = 124\n",
        "def token2bucket_id(token):\n",
        "    return int(_hash(token),16) % NUM_BUCKETS\n",
        "\n",
        "# PLACE YOUR CODE HERE\n",
        "\n",
        "# מחזיר רשימה של כל המילים בצורה הבאה : [(key:token , value:[(doc_id,tf),(doc_id,tf)])] RDD\n",
        "def word_count(text, id):\n",
        "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "    filtered_tokens = [token for token in tokens if token not in all_stopwords]\n",
        "    tf_dict = {}\n",
        "    for token in filtered_tokens:\n",
        "        if token in tf_dict:\n",
        "            tf_dict[token] += 1\n",
        "        else:\n",
        "            tf_dict[token] = 1\n",
        "    return [(token, (id, tf)) for token, tf in tf_dict.items()]\n",
        "\n",
        "# לכל value מהרשימה הקודמת הוא מבצע מיון לפי doc_id\n",
        "def reduce_word_counts(unsorted_pl):\n",
        "    return sorted(unsorted_pl, key=lambda x: x[0])\n",
        "\n",
        "\n",
        "def calculate_df(postings):\n",
        "    return postings.mapValues(lambda posting_list: len(posting_list))\n",
        "\n",
        "\n",
        "def partition_postings_and_write(postings):\n",
        "    # Partition postings into buckets using token2bucket_id\n",
        "    bucketed_postings = postings.map(lambda x: (token2bucket_id(x[0]), (x[0], x[1])))\n",
        "\n",
        "    # Group by bucket ID to get all posting lists in each bucket\n",
        "    grouped_postings = bucketed_postings.groupByKey()\n",
        "\n",
        "    # Step 3: Process each bucket, convert ResultIterable to a list, and write to disk\n",
        "    posting_locations = grouped_postings.map(lambda b_w_pl: InvertedIndex.write_a_posting_list(b_w_pl,bucket_name))\n",
        "    return posting_locations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55c8764e",
      "metadata": {
        "id": "55c8764e",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-index_construction",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "outputId": "8f9687ff-66fc-4b2e-824a-5c376e473bf3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# time the index creation time\n",
        "t_start = time()\n",
        "# word counts map\n",
        "word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
        "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "# filtering postings and calculate df\n",
        "postings_filtered = postings.filter(lambda x: len(x[1])>50)\n",
        "w2df = calculate_df(postings_filtered)\n",
        "w2df_dict = w2df.collectAsMap()\n",
        "# partition posting lists and write out\n",
        "_ = partition_postings_and_write(postings_filtered).collect()\n",
        "index_const_time = time() - t_start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dbc0e14",
      "metadata": {
        "id": "3dbc0e14",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-index_const_time",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# test index construction time\n",
        "assert index_const_time < 60*120"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1fba72e",
      "metadata": {
        "id": "f1fba72e",
        "outputId": "c66d270c-7c00-4959-9337-f3d26ac25a1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3397.397433280945"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index_const_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab3296f4",
      "metadata": {
        "id": "ab3296f4",
        "nbgrader": {
          "grade": true,
          "grade_id": "collect-posting",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# collect all posting lists locations into one super-set\n",
        "super_posting_locs = defaultdict(list)\n",
        "for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
        "  if not blob.name.endswith(\"pickle\"):\n",
        "    continue\n",
        "  with blob.open(\"rb\") as f:\n",
        "    posting_locs = pickle.load(f)\n",
        "    for k, v in posting_locs.items():\n",
        "      super_posting_locs[k].extend(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f66e3a",
      "metadata": {
        "id": "f6f66e3a"
      },
      "source": [
        "Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5d2cfb6",
      "metadata": {
        "id": "a5d2cfb6",
        "outputId": "bd5a9ffc-ad16-4e15-cd14-f8e24260b4ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying file://index.pkl [Content-Type=application/octet-stream]...\n",
            "/ [1 files][ 18.4 MiB/ 18.4 MiB]                                                \n",
            "Operation completed over 1 objects/18.4 MiB.                                     \n"
          ]
        }
      ],
      "source": [
        "# Create inverted index instance\n",
        "inverted = InvertedIndex()\n",
        "# Adding the posting locations dictionary to the inverted index\n",
        "inverted.posting_locs = super_posting_locs\n",
        "# Add the token - df dictionary to the inverted index\n",
        "inverted.df = w2df_dict\n",
        "# write the global stats out\n",
        "inverted.write_index('.', 'index')\n",
        "# upload to gs\n",
        "index_src = \"index.pkl\"\n",
        "index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
        "!gsutil cp $index_src $index_dst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f880d59",
      "metadata": {
        "id": "8f880d59",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-index_dst_size",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "ac30cd86-cff4-43e7-daf8-0f501b50b1b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 18.45 MiB  2025-01-01T08:47:07Z  gs://315347757work3ir/postings_gcp/index.pkl\r\n",
            "TOTAL: 1 objects, 19344222 bytes (18.45 MiB)\r\n"
          ]
        }
      ],
      "source": [
        "!gsutil ls -lh $index_dst"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c52dee14",
      "metadata": {
        "id": "c52dee14",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-2a6d655c112e79c5",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# PageRank"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0875c6bd",
      "metadata": {
        "id": "0875c6bd",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-2fee4bc8d83c1e2a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "**YOUR TASK (10 POINTS):** Compute PageRank for the entire English Wikipedia. Use your implementation for `generate_graph` function from Colab below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31a516e2",
      "metadata": {
        "id": "31a516e2"
      },
      "outputs": [],
      "source": [
        "# Put your `generate_graph` function here\n",
        "def generate_graph(pages):\n",
        "    # Extract edges: (source_id, destination_id)\n",
        "    edges = pages.flatMap(lambda row: [(row['id'], link['id']) for link in row['anchor_text']])\n",
        "\n",
        "    # Remove duplicate edges\n",
        "    edges = edges.distinct()\n",
        "\n",
        "    # Extract vertices: all unique IDs from both sources and destinations\n",
        "    vertices = edges.flatMap(lambda edge: [edge[0], edge[1]]).distinct()\n",
        "\n",
        "    return edges, vertices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bc05ba3",
      "metadata": {
        "id": "6bc05ba3",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-PageRank",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "outputId": "e0472fda-3870-4951-f12c-786ff342140b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/01/01 10:38:09 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_22 !\n",
            "25/01/01 10:38:09 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_64 !\n",
            "25/01/01 10:38:09 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_110 !\n",
            "25/01/01 10:38:09 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_1 !\n",
            "25/01/01 10:38:09 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_74 !\n",
            "25/01/01 10:38:09 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_99 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_109 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_110 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_31 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_39 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_56 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_23 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_105 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_99 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_30 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_118 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_106 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_118 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_14 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_123 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_91 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_100_102 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_9 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_90 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_40 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_119 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_100_108 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_9 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_56 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_64 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_39 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_99 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_22 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_39 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_77 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_106 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_91 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_22 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_14 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_63 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_63 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_119 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_31 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_49 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_110 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_76 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_105 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_57 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_63 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_122 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_48 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_123 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_9 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_75 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_40 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_110 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_90_110 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_56 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_31 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_123 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_106 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_75 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_23 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_48 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_90_108 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_30 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_56 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_30 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_123 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_1 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_109 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_57 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_86_123 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_119 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_100_123 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_98 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_122 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_102 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_76 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_1 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_98 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_64 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_40 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_14 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_48 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_75 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_108 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_23 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_74 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_31 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_77 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_22 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_105 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_98 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_122 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_57 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_14 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_39 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_77 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_86_108 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_118 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_75 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_5 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_90 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_9 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_90 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_119 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_90 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_122 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_74 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_123 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_30 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_109 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_90_123 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_74 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_99 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_5 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_91 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_23 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_86_110 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_5 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_98 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_76 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_100 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_118 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_1 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_5 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_49 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_100_110 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_57 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_40 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_106 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_49 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_110 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_76 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_105 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_48 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_91 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_77 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_49 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_63 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_64 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_109 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_42 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_68 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_70 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_58 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_13 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_12 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_84 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_83 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_58 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_58 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_44 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_13 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_84 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_111 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_70 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_2 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_28 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_87 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_41 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_94 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_107 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_83 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_102 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_60 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_68 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_86_119 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_84 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_41 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_29 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_61 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_70 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_41 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_12 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_59 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_90_103 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_68 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_103 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_107 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_114 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_29 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_94 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_83 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_119 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_61 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_86_103 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_86_105 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_42 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_87 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_111 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_90_119 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_94 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_60 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_21 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_60 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_84 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_21 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_114 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_2 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_90_105 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_41 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_92 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_45 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_12 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_13 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_2 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_21 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_68 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_102 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_28 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_84 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_70 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_114 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_12 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_45 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_44 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_6 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_44 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_111 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_87 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_61 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_13 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_94 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_13 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_70 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_83 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_28 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_114 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_92 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_18 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_107 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_44 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_60 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_107 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_87 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_107 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_18 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_59 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_80 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_45 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_61 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_2 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_28 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_61 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_80 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_42 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_29 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_28 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_21 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_41 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_100_103 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_102 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_58 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_92 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_80 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_111 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_59 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_68 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_18 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_59 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_6 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_96 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_100_119 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_45 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_6 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_83 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_102 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_92 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_59 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_6 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_12 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_18 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_44 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_105 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_42 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_92 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_102 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_111 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_58 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_80 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_29 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_42 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_180_87 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_94 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_60 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_29 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_2 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_6 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_18 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_83_45 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_190_114 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_100_105 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_88_80 !\n",
            "25/01/01 10:38:10 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_98_21 !\n",
            "25/01/01 10:38:13 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1735626877593_0005_01_000007 on host: cluster-0016-w-0.c.ir-work-3.internal. Exit status: 143. Diagnostics: [2025-01-01 10:38:12.154]Container killed on request. Exit code is 143\n",
            "[2025-01-01 10:38:12.155]Container exited with a non-zero exit code 143. \n",
            "[2025-01-01 10:38:12.155]Killed by external signal\n",
            ".\n",
            "25/01/01 10:38:13 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 7 for reason Container from a bad node: container_1735626877593_0005_01_000007 on host: cluster-0016-w-0.c.ir-work-3.internal. Exit status: 143. Diagnostics: [2025-01-01 10:38:12.154]Container killed on request. Exit code is 143\n",
            "[2025-01-01 10:38:12.155]Container exited with a non-zero exit code 143. \n",
            "[2025-01-01 10:38:12.155]Killed by external signal\n",
            ".\n",
            "25/01/01 10:38:13 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 7 on cluster-0016-w-0.c.ir-work-3.internal: Container from a bad node: container_1735626877593_0005_01_000007 on host: cluster-0016-w-0.c.ir-work-3.internal. Exit status: 143. Diagnostics: [2025-01-01 10:38:12.154]Container killed on request. Exit code is 143\n",
            "[2025-01-01 10:38:12.155]Container exited with a non-zero exit code 143. \n",
            "[2025-01-01 10:38:12.155]Killed by external signal\n",
            ".\n",
            "25/01/01 10:38:13 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 13.0 in stage 170.0 (TID 4412) (cluster-0016-w-0.c.ir-work-3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_1735626877593_0005_01_000007 on host: cluster-0016-w-0.c.ir-work-3.internal. Exit status: 143. Diagnostics: [2025-01-01 10:38:12.154]Container killed on request. Exit code is 143\n",
            "[2025-01-01 10:38:12.155]Container exited with a non-zero exit code 143. \n",
            "[2025-01-01 10:38:12.155]Killed by external signal\n",
            ".\n",
            "25/01/01 10:38:13 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 12.0 in stage 170.0 (TID 4409) (cluster-0016-w-0.c.ir-work-3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_1735626877593_0005_01_000007 on host: cluster-0016-w-0.c.ir-work-3.internal. Exit status: 143. Diagnostics: [2025-01-01 10:38:12.154]Container killed on request. Exit code is 143\n",
            "[2025-01-01 10:38:12.155]Container exited with a non-zero exit code 143. \n",
            "[2025-01-01 10:38:12.155]Killed by external signal\n",
            ".\n",
            "25/01/01 10:38:17 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1735626877593_0005_01_000002 on host: cluster-0016-w-1.c.ir-work-3.internal. Exit status: 143. Diagnostics: [2025-01-01 10:38:17.076]Container killed on request. Exit code is 143\n",
            "[2025-01-01 10:38:17.077]Container exited with a non-zero exit code 143. \n",
            "[2025-01-01 10:38:17.077]Killed by external signal\n",
            ".\n",
            "25/01/01 10:38:17 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 2 for reason Container from a bad node: container_1735626877593_0005_01_000002 on host: cluster-0016-w-1.c.ir-work-3.internal. Exit status: 143. Diagnostics: [2025-01-01 10:38:17.076]Container killed on request. Exit code is 143\n",
            "[2025-01-01 10:38:17.077]Container exited with a non-zero exit code 143. \n",
            "[2025-01-01 10:38:17.077]Killed by external signal\n",
            ".\n",
            "25/01/01 10:38:17 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 2 on cluster-0016-w-1.c.ir-work-3.internal: Container from a bad node: container_1735626877593_0005_01_000002 on host: cluster-0016-w-1.c.ir-work-3.internal. Exit status: 143. Diagnostics: [2025-01-01 10:38:17.076]Container killed on request. Exit code is 143\n",
            "[2025-01-01 10:38:17.077]Container exited with a non-zero exit code 143. \n",
            "[2025-01-01 10:38:17.077]Killed by external signal\n",
            ".\n",
            "25/01/01 10:38:17 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 11.0 in stage 170.0 (TID 4380) (cluster-0016-w-1.c.ir-work-3.internal executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1735626877593_0005_01_000002 on host: cluster-0016-w-1.c.ir-work-3.internal. Exit status: 143. Diagnostics: [2025-01-01 10:38:17.076]Container killed on request. Exit code is 143\n",
            "[2025-01-01 10:38:17.077]Container exited with a non-zero exit code 143. \n",
            "[2025-01-01 10:38:17.077]Killed by external signal\n",
            ".\n",
            "25/01/01 10:38:17 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 10.0 in stage 170.0 (TID 4379) (cluster-0016-w-1.c.ir-work-3.internal executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1735626877593_0005_01_000002 on host: cluster-0016-w-1.c.ir-work-3.internal. Exit status: 143. Diagnostics: [2025-01-01 10:38:17.076]Container killed on request. Exit code is 143\n",
            "[2025-01-01 10:38:17.077]Container exited with a non-zero exit code 143. \n",
            "[2025-01-01 10:38:17.077]Killed by external signal\n",
            ".\n",
            "[Stage 338:====================================================>(198 + 2) / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+\n",
            "|     id|          pagerank|\n",
            "+-------+------------------+\n",
            "|3434750|  9913.72878216078|\n",
            "|  10568| 5385.349263642037|\n",
            "|  32927| 5282.081575765277|\n",
            "|  30680|  5128.23370960412|\n",
            "|5843419|  4957.56768626387|\n",
            "|  68253|  4769.27826535516|\n",
            "|  31717|  4486.35018054831|\n",
            "|  11867|  4146.41465091277|\n",
            "|  14533|3996.4664408855033|\n",
            "| 645042| 3531.627089803744|\n",
            "|  17867|3246.0983906041406|\n",
            "|5042916| 2991.945739166179|\n",
            "|4689264|2982.3248830417488|\n",
            "|  14532| 2934.746829203171|\n",
            "|  25391|2903.5462235133973|\n",
            "|   5405|2891.4163291546356|\n",
            "|4764461| 2834.366987332661|\n",
            "|  15573|2783.8651181588384|\n",
            "|   9316|2782.0396464137702|\n",
            "|8569916|2775.2861918400167|\n",
            "+-------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "\n",
        "t_start = time()\n",
        "pages_links = spark.read.parquet(\"gs://315347757work3ir/multistream*\").select(\"id\", \"anchor_text\").rdd\n",
        "# construct the graph\n",
        "edges, vertices = generate_graph(pages_links)\n",
        "# compute PageRank\n",
        "edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n",
        "# vertices_tuples = vertices.map(lambda x: (x,))\n",
        "# verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n",
        "verticesDF = vertices.map(lambda x: (x,)).toDF(['id']).repartition(124, 'id')\n",
        "g = GraphFrame(verticesDF, edgesDF)\n",
        "pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n",
        "pr = pr_results.vertices.select(\"id\", \"pagerank\")\n",
        "pr = pr.sort(col('pagerank').desc())\n",
        "pr.repartition(1).write.csv(f'gs://{bucket_name}/pr', compression=\"gzip\")\n",
        "pr_time = time() - t_start\n",
        "pr.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7717604",
      "metadata": {
        "id": "f7717604",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-PageRank_time",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "a0d8d02c-2f3b-4aa6-8ae2-2e38cb75f8e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7130.495651960373"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# test that PageRank computaion took less than 1 hour\n",
        "assert pr_time < 60*120"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96e9a610",
      "metadata": {
        "id": "96e9a610"
      },
      "source": [
        "# Reporting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1da57c7",
      "metadata": {
        "id": "a1da57c7"
      },
      "source": [
        "**YOUR TASK (5 points):** execute and complete the following lines to complete\n",
        "the reporting requirements for assignment #3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f0d5523",
      "metadata": {
        "id": "0f0d5523",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-size_ofi_input_data",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "54595c29-4ae3-4b78-86d0-d8457ae9c150"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20.27 GiB    gs://315347757work3ir/*\r\n"
          ]
        }
      ],
      "source": [
        "# size of input data\n",
        "!gsutil du -sh \"gs://315347757work3ir/*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce25a98a",
      "metadata": {
        "id": "ce25a98a",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-size_of_index_data",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "44d9721a-1cd7-4e59-9f78-5439864cfdad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.92 GiB     gs://315347757work3ir/postings_gcp\r\n"
          ]
        }
      ],
      "source": [
        "# size of index data\n",
        "index_dst = f'gs://{bucket_name}/postings_gcp/'\n",
        "!gsutil du -sh \"$index_dst\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7a9538ee",
      "metadata": {
        "id": "7a9538ee",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-credits",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d573d2b8-3e87-4795-da5d-ae67c52b02a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I used 50 USD credit during the course of this assignment\n"
          ]
        }
      ],
      "source": [
        "# How many USD credits did you use in GCP during the course of this assignment?\n",
        "cost = 50\n",
        "print(f'I used {cost} USD credit during the course of this assignment')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb0e0ed8",
      "metadata": {
        "id": "fb0e0ed8"
      },
      "source": [
        "**Bonus (10 points)** if you implement PageRank in pure PySpark, i.e. without using the GraphFrames package, AND manage to complete 10 iterations of your algorithm on the entire English Wikipedia in less than an hour.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8157868",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-PageRank_Bonus",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "b8157868"
      },
      "outputs": [],
      "source": [
        "#If you have decided to do the bonus task - please copy the code here\n",
        "\n",
        "bonus_flag = False # Turn flag on (True) if you have implemented this part\n",
        "\n",
        "t_start = time()\n",
        "\n",
        "# PLACE YOUR CODE HERE\n",
        "\n",
        "pr_time_Bonus = time() - t_start\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "855f9c94",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-PageRank_Bonus-time",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "855f9c94"
      },
      "outputs": [],
      "source": [
        "# Note:test that PageRank computaion took less than 1 hour\n",
        "assert pr_time_Bonus < 60*60 and bonus_flag"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pyspark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}